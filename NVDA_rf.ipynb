{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a0c9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/py/pk80p4l95gjbynd00fzl_3d00000gn/T/ipykernel_45628/1138104080.py:8: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  nvda = yf.download('NVDA', start='2015-01-01')\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open     Volume\n",
      "Ticker            NVDA        NVDA        NVDA        NVDA       NVDA\n",
      "Date                                                                 \n",
      "2025-08-28  180.169998  184.470001  176.410004  180.820007  281787800\n",
      "2025-08-29  174.179993  178.149994  173.149994  178.110001  243257900\n",
      "2025-09-02  170.779999  172.380005  167.220001  170.000000  231164900\n",
      "2025-09-03  170.619995  172.410004  168.880005  171.059998  162364400\n",
      "2025-09-04  170.663498  171.860001  169.410004  170.550003   85539257\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_scroll {\n",
       "    overflow-y: scroll;\n",
       "    height: 200px;   /* Adjust height as needed */\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Using the yfinance library to download historical stock data for NVIDIA (NVDA)\n",
    "# From January 1, 2015, to the present day.\n",
    "#!pip install yfinance\n",
    "import yfinance as yf\n",
    "nvda = yf.download('NVDA', start='2015-01-01')  \n",
    "print(nvda.tail())\n",
    "\n",
    "# Checking for missing values in the dataset\n",
    "print(nvda.isna().sum().sum())\n",
    "# As there are no missing values, we can proceed with the analysis.\n",
    "\n",
    "# Importing the technical analysis library\n",
    "#!pip install ta \n",
    "import ta\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".output_scroll {\n",
    "    overflow-y: scroll;\n",
    "    height: 200px;   /* Adjust height as needed */\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12dfb26",
   "metadata": {},
   "source": [
    "<a id=\"adding-features\"></a>\n",
    "## Adding basic features to the dataset to use in our ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14591ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open     Volume Target  \\\n",
      "Ticker            NVDA        NVDA        NVDA        NVDA       NVDA          \n",
      "Date                                                                           \n",
      "2025-08-28  180.169998  184.470001  176.410004  180.820007  281787800      0   \n",
      "2025-08-29  174.179993  178.149994  173.149994  178.110001  243257900      0   \n",
      "2025-09-02  170.779999  172.380005  167.220001  170.000000  231164900      0   \n",
      "2025-09-03  170.619995  172.410004  168.880005  171.059998  162364400      1   \n",
      "2025-09-04  170.663498  171.860001  169.410004  170.550003   85539257      0   \n",
      "\n",
      "Price         Return         MA5        MA10        MA20 Volatility5  \\\n",
      "Ticker                                                                 \n",
      "Date                                                                   \n",
      "2025-08-28 -0.007874  180.268002  178.981999  179.675999    0.010098   \n",
      "2025-08-29 -0.033246  179.506000  178.354999  179.698999    0.018055   \n",
      "2025-09-02 -0.019520  177.700000  177.231999  179.237999    0.016989   \n",
      "2025-09-03 -0.000937  175.469998  176.729999  178.855999    0.013861   \n",
      "2025-09-04  0.000255  173.282697  176.256349  178.418174    0.014117   \n",
      "\n",
      "Price      Volatility10  Momentum5 Momentum10 Close/Open  High/Low  \\\n",
      "Ticker                                                               \n",
      "Date                                                                 \n",
      "2025-08-28     0.014739   5.190002  -1.850006   0.996405  1.045689   \n",
      "2025-08-29     0.017889  -3.810013  -6.270004   0.977935  1.028877   \n",
      "2025-09-02     0.018002  -9.029999 -11.229996   1.004588  1.030858   \n",
      "2025-09-03     0.014903 -11.150009  -5.020004   0.997428  1.020902   \n",
      "2025-09-04     0.014929 -10.936508  -4.736496   1.000665  1.014462   \n",
      "\n",
      "Price      Volume_Change  \n",
      "Ticker                    \n",
      "Date                      \n",
      "2025-08-28      0.196455  \n",
      "2025-08-29     -0.136734  \n",
      "2025-09-02     -0.049713  \n",
      "2025-09-03     -0.297625  \n",
      "2025-09-04     -0.473165  \n",
      "[('Close', 'NVDA'), ('High', 'NVDA'), ('Low', 'NVDA'), ('Open', 'NVDA'), ('Volume', 'NVDA'), ('Target', ''), ('Return', ''), ('MA5', ''), ('MA10', ''), ('MA20', ''), ('Volatility5', ''), ('Volatility10', ''), ('Momentum5', ''), ('Momentum10', ''), ('Close/Open', ''), ('High/Low', ''), ('Volume_Change', '')]\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Up/Down Movement\n",
    "nvda['Target'] = (nvda['Close'].shift(-1) > nvda['Close']).astype(int)\n",
    "\n",
    "nvda['Return'] = nvda['Close'].pct_change()\n",
    "\n",
    "# Rolling Statistics\n",
    "nvda['MA5'] = nvda['Close'].rolling(window=5).mean()\n",
    "nvda['MA10'] = nvda['Close'].rolling(window=10).mean()\n",
    "nvda['MA20'] = nvda['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Volatility\n",
    "nvda['Volatility5'] = nvda['Return'].rolling(window=5).std()\n",
    "nvda['Volatility10'] = nvda['Return'].rolling(window=10).std()\n",
    "\n",
    "# Momentum\n",
    "nvda['Momentum5'] = nvda['Close'] - nvda['Close'].shift(5)\n",
    "nvda['Momentum10'] = nvda['Close'] - nvda['Close'].shift(10)\n",
    "\n",
    "# Price Ratios\n",
    "nvda['Close/Open'] = nvda['Close'] / nvda['Open']\n",
    "nvda['High/Low'] = nvda['High'] / nvda['Low']\n",
    "\n",
    "# Volume Change\n",
    "nvda['Volume_Change'] = nvda['Volume'].pct_change()\n",
    "\n",
    "# Drop rows with NaNs caused by rolling calculations\n",
    "nvda.dropna(inplace=True)\n",
    "\n",
    "# Check the final dataset with features\n",
    "print(nvda.tail())\n",
    "\n",
    "# Displaying the columns to verify the features added\n",
    "print(nvda.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b1709",
   "metadata": {},
   "source": [
    "## Model Choice: Random Forest\n",
    "\n",
    "I’m starting with Random Forest because it’s a solid, easy-to-use model that works well with different types of features and can capture complex patterns. It’s less likely to overfit compared to single decision trees and doesn’t need a lot of tuning upfront. Plus, it gives insight into which features matter most, which is helpful when exploring financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a288d187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.90      0.59       238\n",
      "           1       0.52      0.09      0.15       295\n",
      "\n",
      "    accuracy                           0.45       533\n",
      "   macro avg       0.48      0.49      0.37       533\n",
      "weighted avg       0.49      0.45      0.35       533\n",
      "\n",
      "Train scores: [0.84269663 0.81214848 0.75168792 0.73157006 0.72264746]\n",
      "Validation scores: [0.44594595 0.51126126 0.47747748 0.47747748 0.4481982 ]\n",
      "Mean train accuracy: 0.7721501101273472\n",
      "Mean validation accuracy: 0.472072072072072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Features and target\n",
    "# I have removed Date, raw price columns from Features\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "nvda.columns = ['_'.join(filter(None, col)).strip() for col in nvda.columns.values]\n",
    "\n",
    "X = nvda[\n",
    "    ['Return', 'MA5', 'MA10', 'MA20',\n",
    "     'Volatility5', 'Volatility10',\n",
    "     'Momentum5', 'Momentum10',\n",
    "     'Close/Open', 'High/Low',\n",
    "     'Volume_NVDA']\n",
    "]\n",
    "y = nvda['Target']\n",
    "\n",
    "# Train/test split (no shuffle to keep time order)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Initialize Random Forest with baseline parameters\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # helps if classes are imbalanced\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Do 5-fold time-series-split cross-validation to get better accuracy scores, Test for over/underfitting\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate\n",
    "\n",
    "# Define model\n",
    "rf_1 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'         \n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(rf_1, X, y, cv=TimeSeriesSplit(n_splits=5), scoring='accuracy', return_train_score=True)\n",
    "\n",
    "print(\"Train scores:\", cv_results['train_score'])\n",
    "print(\"Validation scores:\", cv_results['test_score'])\n",
    "\n",
    "print(\"Mean train accuracy:\", np.mean(cv_results['train_score']))\n",
    "print(\"Mean validation accuracy:\", np.mean(cv_results['test_score']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bab1ed",
   "metadata": {},
   "source": [
    "As you can see here we have an overfitting problem. I am going to adjust the model to try and reduce the overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd85770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scores: [0.6988764  0.65241845 0.63990998 0.62521103 0.61728951]\n",
      "Validation scores: [0.45495495 0.50900901 0.45495495 0.47522523 0.4481982 ]\n",
      "Mean train accuracy: 0.6467410737476837\n",
      "Mean validation accuracy: 0.46846846846846846\n"
     ]
    }
   ],
   "source": [
    "rf_2 = RandomForestClassifier(\n",
    "    n_estimators=100,          # Keep enough trees for stability\n",
    "    max_depth=3,               # Shallower trees to reduce complexity\n",
    "    min_samples_split=20,      # Require larger node size to split\n",
    "    min_samples_leaf=10,       # Minimum leaf size to prevent small noisy leaves\n",
    "    max_features='sqrt',       # Use sqrt of features at each split (common practice)\n",
    "    random_state=42,\n",
    "    class_weight='balanced'    # Handle class imbalance if any\n",
    ")\n",
    "\n",
    "cv_results_2 = cross_validate(rf_2, X, y, cv=TimeSeriesSplit(n_splits=5), scoring='accuracy', return_train_score=True)\n",
    "\n",
    "print(\"Train scores:\", cv_results_2['train_score'])\n",
    "print(\"Validation scores:\", cv_results_2['test_score'])\n",
    "\n",
    "print(\"Mean train accuracy:\", np.mean(cv_results_2['train_score']))\n",
    "print(\"Mean validation accuracy:\", np.mean(cv_results_2['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410c65b",
   "metadata": {},
   "source": [
    "We have reduced overfitting slightly, however validation accuracy remains low at ~52%. Let's now focus on improving this. I will begin by adding a wide range of potentially relevant features to the model, and then carry out feature importance analysis to aggressively reduce to only the most impactful ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a04107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scores: [0.89342404 0.83900227 0.80801209 0.78401361 0.76780045]\n",
      "Validation scores: [0.53968254 0.48526077 0.49659864 0.46938776 0.45804989]\n",
      "Mean train accuracy: 0.818450491307634\n",
      "Mean validation accuracy: 0.4897959183673469\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Alias for convenience with suffixed columns\n",
    "close = nvda['Close_NVDA']\n",
    "high = nvda['High_NVDA']\n",
    "low = nvda['Low_NVDA']\n",
    "open_ = nvda['Open_NVDA']\n",
    "volume = nvda['Volume_NVDA']\n",
    "\n",
    "# Lagged Returns - recent return history to capture momentum\n",
    "nvda['Return_1'] = nvda['Return'].shift(1)\n",
    "nvda['Return_2'] = nvda['Return'].shift(2)\n",
    "nvda['Return_3'] = nvda['Return'].shift(3)\n",
    "\n",
    "# Daily Range - intraday volatility measure\n",
    "nvda['Daily_Range'] = high - low\n",
    "\n",
    "# Rolling Max/Min - price extremes over past 10 days\n",
    "nvda['Max10'] = close.rolling(window=10).max()\n",
    "nvda['Min10'] = close.rolling(window=10).min()\n",
    "nvda['Close_to_High10'] = close / nvda['Max10']\n",
    "nvda['Close_to_Low10'] = close / nvda['Min10']\n",
    "\n",
    "# Price Deviation from Moving Averages - mean reversion signals\n",
    "nvda['Dev_MA5'] = close - nvda['MA5']\n",
    "nvda['Dev_MA10'] = close - nvda['MA10']\n",
    "\n",
    "# Rolling Average Volume and Volume Spike ratio\n",
    "nvda['AvgVol5'] = volume.rolling(window=5).mean()\n",
    "nvda['Vol/AvgVol5'] = volume / nvda['AvgVol5']  # large values could indicate unusual activity\n",
    "\n",
    "# RSI (Relative Strength Index) - momentum and reversal indicator\n",
    "delta = close.diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "rs = gain / loss\n",
    "nvda['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# MACD and Signal Line - trend following and momentum\n",
    "ema12 = close.ewm(span=12, adjust=False).mean()\n",
    "ema26 = close.ewm(span=26, adjust=False).mean()\n",
    "nvda['MACD'] = ema12 - ema26\n",
    "nvda['Signal_Line'] = nvda['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Candle Body and Wick Ratios - indicate market sentiment within day\n",
    "nvda['Body'] = abs(close - open_)\n",
    "nvda['Upper_Wick'] = high - np.maximum(close, open_)\n",
    "nvda['Lower_Wick'] = np.minimum(close, open_) - low\n",
    "nvda['Body/Range'] = nvda['Body'] / nvda['Daily_Range']\n",
    "\n",
    "# Volume Price Trend (VPT) - cumulative volume weighted price change, volume + price action\n",
    "nvda['VPT'] = (volume * close.pct_change()).cumsum()\n",
    "\n",
    "# Bollinger Bands - volatility bands around MA20\n",
    "nvda['MA20'] = close.rolling(window=20).mean()  # just in case not added yet\n",
    "nvda['BB_upper'] = nvda['MA20'] + 2 * close.rolling(window=20).std()\n",
    "nvda['BB_lower'] = nvda['MA20'] - 2 * close.rolling(window=20).std()\n",
    "nvda['BB_width'] = nvda['BB_upper'] - nvda['BB_lower']  # width as volatility measure\n",
    "\n",
    "# Stochastic Oscillator (%K and %D) - momentum oscillator\n",
    "lowest_low_14 = low.rolling(window=14).min()\n",
    "highest_high_14 = high.rolling(window=14).max()\n",
    "nvda['Stoch_%K'] = 100 * (close - lowest_low_14) / (highest_high_14 - lowest_low_14)\n",
    "nvda['Stoch_%D'] = nvda['Stoch_%K'].rolling(window=3).mean()\n",
    "\n",
    "# ATR (Average True Range) - volatility measure based on high-low and gaps\n",
    "high_low = high - low\n",
    "high_close_prev = abs(high - close.shift(1))\n",
    "low_close_prev = abs(low - close.shift(1))\n",
    "true_range = pd.Series(np.maximum.reduce([high_low, high_close_prev, low_close_prev]), index=nvda.index)\n",
    "nvda['ATR'] = true_range.rolling(window=14).mean()\n",
    "\n",
    "# Momentum Indicators - price change over different lags (some you have but adding more)\n",
    "nvda['Momentum_3'] = close - close.shift(3)\n",
    "nvda['Momentum_7'] = close - close.shift(7)\n",
    "nvda['Momentum_14'] = close - close.shift(14)\n",
    "\n",
    "# Volume Moving Averages - different windows to capture volume trends\n",
    "nvda['AvgVol10'] = volume.rolling(window=10).mean()\n",
    "nvda['AvgVol20'] = volume.rolling(window=20).mean()\n",
    "\n",
    "# Volume Oscillator - difference between short and long volume MAs\n",
    "nvda['Vol_Oscillator'] = nvda['AvgVol5'] - nvda['AvgVol20']\n",
    "\n",
    "# Drop NaNs caused by rolling and shifting\n",
    "nvda.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Carrying out RF with new features\n",
    "X_3 = nvda.drop(columns=['Target', 'Close_NVDA', 'High_NVDA', 'Low_NVDA', 'Open_NVDA', 'Volume_NVDA'])\n",
    "y_3 = nvda['Target']\n",
    "\n",
    "rf_3 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "cv_results_3 = cross_validate(rf_3, X_3, y_3, cv=TimeSeriesSplit(n_splits=5), scoring='accuracy', return_train_score=True)\n",
    "\n",
    "print(\"Train scores:\", cv_results_3['train_score'])\n",
    "print(\"Validation scores:\", cv_results_3['test_score'])\n",
    "\n",
    "print(\"Mean train accuracy:\", np.mean(cv_results_3['train_score']))\n",
    "print(\"Mean validation accuracy:\", np.mean(cv_results_3['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a40adf",
   "metadata": {},
   "source": [
    "<a id=\"feature-importance-analysis\"></a>\n",
    "\n",
    "# Feature Importance Analysis\n",
    "We will start by looking for high magnitude to find redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bab7cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Feature  Count\n",
      "9      BB_upper      8\n",
      "14          ATR      8\n",
      "1           MA5      7\n",
      "10     BB_lower      7\n",
      "0          MA10      7\n",
      "4         Min10      7\n",
      "3         Max10      7\n",
      "2          MA20      7\n",
      "5      Dev_MA10      2\n",
      "11     BB_width      2\n",
      "6     Momentum5      1\n",
      "7   Signal_Line      1\n",
      "8          MACD      1\n",
      "12     Stoch_%D      1\n",
      "13     Stoch_%K      1\n",
      "15   Momentum_3      1\n",
      "16      Dev_MA5      1\n",
      "17   Momentum_7      1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = nvda.drop(columns=['Target','Close_NVDA', 'High_NVDA', 'Low_NVDA', 'Open_NVDA', 'Volume_NVDA']).corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix without diagonal\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Set threshold for \"high\" correlation\n",
    "threshold = 0.9\n",
    "\n",
    "# Find pairs with correlation above threshold\n",
    "high_corr_pairs = [(col, row, upper.loc[row, col]) \n",
    "                   for col in upper.columns \n",
    "                   for row in upper.index \n",
    "                   if (upper.loc[row, col] > threshold)]\n",
    "\n",
    "# Convert to DataFrame for nicer display\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Feature 1', 'Feature 2', 'Correlation'])\n",
    "\n",
    "#print(high_corr_df)\n",
    "\n",
    "### Due to the large number of features, we will summarize the most common features in high correlation pairs\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten list of feature names from pairs (both Feature 1 and Feature 2)\n",
    "features_in_pairs = [feat for pair in high_corr_pairs for feat in pair[:2]]\n",
    "\n",
    "# Count appearances\n",
    "feature_counts = Counter(features_in_pairs)\n",
    "\n",
    "# Convert to DataFrame for nicer display\n",
    "feature_counts_df = pd.DataFrame(feature_counts.items(), columns=['Feature', 'Count']).sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(feature_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b3413",
   "metadata": {},
   "source": [
    "As expected there are lots of correlations due to lots of different features calculated using each other. I will drop a few features with lots of correlation and see if validation accuracy improves with less noise and fewer overlapping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f30e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scores: [0.90929705 0.82653061 0.80725624 0.78798186 0.77188209]\n",
      "Validation scores: [0.53287982 0.48072562 0.4829932  0.48979592 0.4829932 ]\n",
      "Mean train accuracy: 0.8205895691609978\n",
      "Mean validation accuracy: 0.49387755102040815\n"
     ]
    }
   ],
   "source": [
    "# Based on the correlation analysis, we will drop some features to reduce redundancy\n",
    "\n",
    "X_4 = nvda.drop(columns=['Target', 'Close_NVDA', 'High_NVDA', 'Low_NVDA', 'Open_NVDA', 'Volume_NVDA',    \n",
    "                         'MA5',          # Keep MA10 instead (middle ground smoothing)\n",
    "                         'Max10',        # Drop price extremes (optional, keep Min10 if you want)\n",
    "                         'BB_upper',     # Bollinger upper band overlaps with MA and price\n",
    "                         'BB_lower'      # Bollinger lower band overlaps with MA and price\n",
    "                        ])\n",
    "y_4 = nvda['Target']\n",
    "\n",
    "rf_4 = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "cv_results_4 = cross_validate(rf_4, X_4, y_4, cv=TimeSeriesSplit(n_splits=5), scoring='accuracy', return_train_score=True)\n",
    "\n",
    "print(\"Train scores:\", cv_results_4['train_score'])\n",
    "print(\"Validation scores:\", cv_results_4['test_score'])\n",
    "\n",
    "print(\"Mean train accuracy:\", np.mean(cv_results_4['train_score']))\n",
    "print(\"Mean validation accuracy:\", np.mean(cv_results_4['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55ca0c",
   "metadata": {},
   "source": [
    "That did not do much: time for some feature importance analysis. We’ll start with Permutation Importance (quick and easy), then dive into SHAP for deeper interpretability. Finally, we’ll compare both and drop or tweak any weak features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1cc031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking by permutation importance:\n",
      "Vol_Oscillator: 0.0228 +/- 0.0030\n",
      "Return_2: 0.0215 +/- 0.0034\n",
      "Volatility10: 0.0205 +/- 0.0035\n",
      "Lower_Wick: 0.0182 +/- 0.0034\n",
      "Upper_Wick: 0.0176 +/- 0.0027\n",
      "Body/Range: 0.0163 +/- 0.0026\n",
      "Momentum_7: 0.0158 +/- 0.0031\n",
      "Close/Open: 0.0155 +/- 0.0030\n",
      "AvgVol20: 0.0155 +/- 0.0049\n",
      "Dev_MA10: 0.0153 +/- 0.0033\n",
      "Return_1: 0.0145 +/- 0.0027\n",
      "Close_to_High10: 0.0136 +/- 0.0017\n",
      "Vol/AvgVol5: 0.0133 +/- 0.0034\n",
      "ATR: 0.0125 +/- 0.0024\n",
      "Return_3: 0.0124 +/- 0.0032\n",
      "Close_to_Low10: 0.0116 +/- 0.0035\n",
      "High/Low: 0.0112 +/- 0.0032\n",
      "Volatility5: 0.0111 +/- 0.0025\n",
      "Stoch_%D: 0.0108 +/- 0.0022\n",
      "RSI: 0.0107 +/- 0.0027\n",
      "Body: 0.0105 +/- 0.0017\n",
      "Return: 0.0105 +/- 0.0036\n",
      "Momentum_14: 0.0102 +/- 0.0028\n",
      "MACD: 0.0091 +/- 0.0020\n",
      "MA20: 0.0090 +/- 0.0020\n",
      "Volume_Change: 0.0087 +/- 0.0025\n",
      "Momentum5: 0.0087 +/- 0.0017\n",
      "Signal_Line: 0.0080 +/- 0.0027\n",
      "Dev_MA5: 0.0077 +/- 0.0026\n",
      "BB_width: 0.0077 +/- 0.0022\n",
      "Daily_Range: 0.0069 +/- 0.0023\n",
      "AvgVol5: 0.0069 +/- 0.0022\n",
      "MA10: 0.0069 +/- 0.0024\n",
      "Stoch_%K: 0.0060 +/- 0.0022\n",
      "VPT: 0.0052 +/- 0.0011\n",
      "Min10: 0.0051 +/- 0.0027\n",
      "AvgVol10: 0.0050 +/- 0.0021\n",
      "Momentum10: 0.0042 +/- 0.0033\n",
      "Momentum_3: 0.0028 +/- 0.0034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit rf_4 on the full data first\n",
    "rf_4.fit(X_4, y_4)\n",
    "\n",
    "# Calculate Permutation Importance on the training data\n",
    "result = permutation_importance(rf_4, X_4, y_4, n_repeats=10, random_state=42, scoring='accuracy')\n",
    "\n",
    "# Extract importance and feature names\n",
    "importances = result.importances_mean\n",
    "std = result.importances_std\n",
    "features = X_4.columns\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"Feature ranking by permutation importance:\")\n",
    "for i in indices:\n",
    "    print(f\"{features[i]}: {importances[i]:.4f} +/- {std[i]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9eecb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHAP Values Verification ===\n",
      "SHAP values shape: (2646, 39, 2)\n",
      "X_4 shape: (2646, 39)\n",
      "Shape match: False\n"
     ]
    }
   ],
   "source": [
    "#!pip install shap\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Create SHAP Explainer\n",
    "# For tree-based models, use TreeExplainer for exact SHAP values\n",
    "explainer = shap.TreeExplainer(rf_4)\n",
    "\n",
    "# 2. Calculate SHAP values\n",
    "# This may take some time for large datasets\n",
    "shap_values = explainer.shap_values(X_4)\n",
    "\n",
    "# For binary classification, shap_values might be a list\n",
    "# Take values for positive class (usually index 1)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "# 3. Get expected value (base value)\n",
    "expected_value = explainer.expected_value\n",
    "if isinstance(expected_value, list):\n",
    "    expected_value = expected_value[1]\n",
    "\n",
    "# 4. Verify SHAP values\n",
    "print(\"=== SHAP Values Verification ===\")\n",
    "\n",
    "# Check shape\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"X_4 shape: {X_4.shape}\")\n",
    "print(f\"Shape match: {shap_values.shape == X_4.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b4c73",
   "metadata": {},
   "source": [
    "Combining permutation importance and SHAP to remove unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cef943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature_names: 39\n",
      "After calculation - shap_importance length: 39\n",
      "=== FEATURE REMOVAL SUMMARY ===\n",
      "\n",
      "Thresholds used:\n",
      "  Permutation importance: < 0.007690854119425522\n",
      "  SHAP importance: < 0.001506\n",
      "\n",
      "Features below SHAP threshold (9):\n",
      "  Close_to_Low10: 0.001480\n",
      "  Daily_Range: 0.001087\n",
      "  Dev_MA5: 0.001480\n",
      "  MA20: 0.001010\n",
      "  Return_1: 0.000973\n",
      "  Return_2: 0.000973\n",
      "  Return_3: 0.001087\n",
      "  Vol_Oscillator: 0.001015\n",
      "  Volatility5: 0.001010\n",
      "\n",
      "Features below permutation threshold (10):\n",
      "  AvgVol10: 0.0050\n",
      "  AvgVol5: 0.0069\n",
      "  BB_width: 0.0077\n",
      "  Daily_Range: 0.0069\n",
      "  MA10: 0.0069\n",
      "  Min10: 0.0051\n",
      "  Momentum10: 0.0042\n",
      "  Momentum_3: 0.0028\n",
      "  Stoch_%K: 0.0060\n",
      "  VPT: 0.0052\n",
      "\n",
      "=== FEATURES TO REMOVE (unimportant in BOTH) ===\n",
      "Total: 1 features\n",
      "  Daily_Range: Perm=0.0069, SHAP=0.001087\n"
     ]
    }
   ],
   "source": [
    "# 1. Get feature names\n",
    "feature_names = X_4.columns.tolist() if hasattr(X_4, 'columns') else [f'Feature_{i}' for i in range(X_4.shape[1])]\n",
    "print(f\"Length of feature_names: {len(feature_names)}\")\n",
    "\n",
    "# 2. Calculate SHAP importance per feature\n",
    "# Reshape if needed (handle flattened array)\n",
    "if len(shap_values.shape) == 1 and len(shap_values) == X_4.shape[0] * X_4.shape[1]:\n",
    "    shap_values = shap_values.reshape(X_4.shape[0], X_4.shape[1])\n",
    "\n",
    "shap_importance = np.mean(np.abs(shap_values), axis=0)\n",
    "print(f\"After calculation - shap_importance length: {len(shap_importance)}\")\n",
    "\n",
    "\n",
    "# 3. Get permutation importance\n",
    "perm_importance = result.importances_mean\n",
    "\n",
    "# 4. Set thresholds (adjust these based on your needs)\n",
    "perm_threshold = np.percentile(perm_importance, 25)  # Bottom 25%\n",
    "shap_threshold = np.percentile(shap_importance, 25)\n",
    "\n",
    "# 5. Find unimportant features\n",
    "unimportant_perm = set([feature_names[i] for i, imp in enumerate(perm_importance) if imp < perm_threshold])\n",
    "shap_importance = np.array(shap_importance).flatten()  # Ensure it's a 1D numpy array\n",
    "unimportant_shap = set([name for name, imp in zip(feature_names, shap_importance) if imp < shap_threshold])\n",
    "\n",
    "# 6. Features to remove (unimportant in BOTH methods)\n",
    "features_to_remove = unimportant_perm.intersection(unimportant_shap)\n",
    "\n",
    "print(\"=== FEATURE REMOVAL SUMMARY ===\")\n",
    "print(f\"\\nThresholds used:\")\n",
    "print(f\"  Permutation importance: < {perm_threshold}\")\n",
    "print(f\"  SHAP importance: < {shap_threshold:.6f}\")\n",
    "\n",
    "print(f\"\\nFeatures below SHAP threshold ({len(unimportant_shap)}):\")\n",
    "for feat in sorted(unimportant_shap):\n",
    "    idx = feature_names.index(feat)\n",
    "    print(f\"  {feat}: {shap_importance[idx]:.6f}\")\n",
    "\n",
    "print(f\"\\nFeatures below permutation threshold ({len(unimportant_perm)}):\")\n",
    "for feat in sorted(unimportant_perm):\n",
    "    idx = feature_names.index(feat)\n",
    "    print(f\"  {feat}: {perm_importance[idx]:.4f}\")\n",
    "\n",
    "print(f\"\\n=== FEATURES TO REMOVE (unimportant in BOTH) ===\")\n",
    "print(f\"Total: {len(features_to_remove)} features\")\n",
    "for feat in sorted(features_to_remove):\n",
    "    idx = feature_names.index(feat)\n",
    "    print(f\"  {feat}: Perm={perm_importance[idx]:.4f}, SHAP={shap_importance[idx]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3582d",
   "metadata": {},
   "source": [
    "# Forward Selection\n",
    "\n",
    "Permutation importance and SHAP values seem to be very disconnected from each other, making it hard to use both to find values to remove. \n",
    "\n",
    "Hence I have decided to try a forward selection instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b9eae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Hide cell output as it is very long\n",
    "\n",
    "!pip install mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Prepare your data\n",
    "# Remove base columns that should never be included\n",
    "base_drops = ['Target', 'Close_NVDA', 'High_NVDA', 'Low_NVDA', 'Open_NVDA', \n",
    "              'Volume_NVDA', 'MA5', 'Max10', 'BB_upper', 'BB_lower']\n",
    "\n",
    "X_for_selection = nvda.drop(columns=base_drops)\n",
    "y_for_selection = nvda['Target']\n",
    "\n",
    "# Get feature names for interpretation\n",
    "feature_names_selection = X_for_selection.columns.tolist()\n",
    "\n",
    "print(f\"Starting feature selection with {len(feature_names_selection)} features...\")\n",
    "print(f\"This may take a few minutes...\\n\")\n",
    "\n",
    "# 1. Forward Selection - Start with 0, add best features one by one\n",
    "sfs_forward = SFS(\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=rf_4.n_estimators,\n",
    "        max_depth=rf_4.max_depth,\n",
    "        random_state=rf_4.random_state,\n",
    "        n_jobs=1  # Set to 1 within SFS to avoid conflicts\n",
    "    ),\n",
    "    k_features='best',  # Automatically find optimal number\n",
    "    forward=True,\n",
    "    floating=False,  # Set True for more thorough search (slower)\n",
    "    scoring='accuracy',\n",
    "    cv=TimeSeriesSplit(n_splits=3),  # Time series validation\n",
    "    n_jobs=-1,  # Parallelize across CV folds\n",
    "    verbose=2  # Show progress\n",
    ")\n",
    "\n",
    "# Fit the selector\n",
    "sfs_forward = sfs_forward.fit(X_for_selection, y_for_selection)\n",
    "\n",
    "# 2. Display results\n",
    "print(\"\\n=== FORWARD SELECTION RESULTS ===\")\n",
    "print(f\"Optimal number of features: {len(sfs_forward.k_feature_names_)}\")\n",
    "print(f\"Best CV score: {sfs_forward.k_score_:.4f}\")\n",
    "print(\"\\nSelected features:\")\n",
    "for feat in sfs_forward.k_feature_names_:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "# 3. Plot the selection process\n",
    "# Get the results for each number of features\n",
    "metric_dict = sfs_forward.get_metric_dict()\n",
    "\n",
    "# Extract data for plotting\n",
    "k_features = []\n",
    "avg_scores = []\n",
    "std_scores = []\n",
    "\n",
    "for k in sorted(metric_dict.keys()):\n",
    "    k_features.append(k)\n",
    "    avg_scores.append(metric_dict[k]['avg_score'])\n",
    "    std_scores.append(metric_dict[k]['std_dev'])\n",
    "\n",
    "# Create performance plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.errorbar(k_features, avg_scores, yerr=std_scores, marker='o', capsize=5)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('CV Accuracy')\n",
    "plt.title('Forward Selection: Accuracy vs Number of Features')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the optimal point\n",
    "optimal_k = len(sfs_forward.k_feature_names_)\n",
    "optimal_score = sfs_forward.k_score_\n",
    "plt.scatter(optimal_k, optimal_score, color='red', s=100, zorder=5, \n",
    "           label=f'Optimal: {optimal_k} features')\n",
    "plt.legend()\n",
    "\n",
    "# 4. Compare with other methods\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Calculate scores for different feature sets\n",
    "methods = {\n",
    "    'All Features': list(feature_names_selection),\n",
    "    'SFS Optimal': list(sfs_forward.k_feature_names_),\n",
    "    'Top 10 SHAP': [feat for feat, _ in sorted(zip(feature_names, shap_importance), \n",
    "                                               key=lambda x: x[1], reverse=True)[:10]],\n",
    "    'Top 15 SHAP': [feat for feat, _ in sorted(zip(feature_names, shap_importance), \n",
    "                                               key=lambda x: x[1], reverse=True)[:15]]\n",
    "}\n",
    "\n",
    "# Remove features not in X_for_selection from SHAP selections\n",
    "methods['Top 10 SHAP'] = [f for f in methods['Top 10 SHAP'] if f in feature_names_selection]\n",
    "methods['Top 15 SHAP'] = [f for f in methods['Top 15 SHAP'] if f in feature_names_selection]\n",
    "\n",
    "method_scores = []\n",
    "method_names = []\n",
    "\n",
    "for method_name, features in methods.items():\n",
    "    X_method = X_for_selection[features]\n",
    "    scores = []\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    for train_idx, test_idx in tscv.split(X_method):\n",
    "        X_train, X_test = X_method.iloc[train_idx], X_method.iloc[test_idx]\n",
    "        y_train, y_test = y_for_selection.iloc[train_idx], y_for_selection.iloc[test_idx]\n",
    "        \n",
    "        rf_temp = RandomForestClassifier(\n",
    "            n_estimators=rf_4.n_estimators,\n",
    "            max_depth=rf_4.max_depth,\n",
    "            random_state=rf_4.random_state\n",
    "        )\n",
    "        rf_temp.fit(X_train, y_train)\n",
    "        scores.append(rf_temp.score(X_test, y_test))\n",
    "    \n",
    "    method_scores.append(np.mean(scores))\n",
    "    method_names.append(f\"{method_name}\\n({len(features)} features)\")\n",
    "\n",
    "# Plot comparison\n",
    "bars = plt.bar(range(len(method_names)), method_scores)\n",
    "plt.xticks(range(len(method_names)), method_names)\n",
    "plt.ylabel('CV Accuracy')\n",
    "plt.title('Feature Selection Method Comparison')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Highlight best method\n",
    "best_idx = np.argmax(method_scores)\n",
    "bars[best_idx].set_color('green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save the selected features for future use\n",
    "selected_features_df = pd.DataFrame({\n",
    "    'feature': list(sfs_forward.k_feature_names_),\n",
    "    'selected': True\n",
    "})\n",
    "#print(\"\\n=== SAVED RESULTS ===\")\n",
    "#print(\"Selected features saved to 'sfs_selected_features.csv'\")\n",
    "#selected_features_df.to_csv('sfs_selected_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bc6e1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scores: [0.66439909 0.65079365 0.6318972  0.63321995 0.61723356]\n",
      "Validation scores: [0.5260771  0.5600907  0.4739229  0.51927438 0.54875283]\n",
      "\n",
      "Mean train accuracy: 0.6395086923658352\n",
      "Mean validation accuracy: 0.5256235827664398\n"
     ]
    }
   ],
   "source": [
    "X_sfs = X_for_selection[list(sfs_forward.k_feature_names_)]  # This is X_for_selection\n",
    "y_sfs = nvda['Target']  # This is y_for_selection\n",
    "\n",
    "# Add regularization to prevent overfitting\n",
    "rf_test = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,           # Limit depth to prevent overfitting\n",
    "    min_samples_split=50,  # Require more samples to split\n",
    "    min_samples_leaf=20,   # Require more samples in leaves\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Use same CV splits\n",
    "cv_results_sfs = cross_validate(\n",
    "    rf_test, X_sfs, y_sfs, \n",
    "    cv=TimeSeriesSplit(n_splits=5),\n",
    "    scoring='accuracy', \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Train scores:\", cv_results_sfs['train_score'])\n",
    "print(\"Validation scores:\", cv_results_sfs['test_score'])\n",
    "print(\"\\nMean train accuracy:\", np.mean(cv_results_sfs['train_score']))\n",
    "print(\"Mean validation accuracy:\", np.mean(cv_results_sfs['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d3fce",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "Now we have decided on features it is time for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Hiding cell output as it is very long\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define what to test\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [10, 30, 50],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Set up the search\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=TimeSeriesSplit(n_splits=3),  # Time series validation!\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=2   # Show progress\n",
    ")\n",
    "\n",
    "# Find best parameters\n",
    "grid_search.fit(X_sfs, y_sfs)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best model\n",
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f030443",
   "metadata": {},
   "source": [
    "Best parameters: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "Best CV accuracy: 0.5497\n",
    "\n",
    "# Results\n",
    "Lets evaluate our final model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b994259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Best Model Performance ===\n",
      "Best parameters: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "\n",
      "Train scores: [0.73469388 0.6462585  0.62887377 0.60827664 0.60362812]\n",
      "Validation scores: [0.51473923 0.55102041 0.53741497 0.52834467 0.57369615]\n",
      "\n",
      "Mean train accuracy: 0.6443\n",
      "Mean validation accuracy: 0.5410\n",
      "Std validation accuracy: 0.0202\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with tuned hyperparameters\n",
    "\n",
    "cv_results_best = cross_validate(\n",
    "    best_rf,  # The best model from grid search\n",
    "    X_sfs, \n",
    "    y_sfs,\n",
    "    cv=TimeSeriesSplit(n_splits=5),  # Same 5-fold time series validation\n",
    "    scoring='accuracy',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"=== Best Model Performance ===\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"\\nTrain scores: {cv_results_best['train_score']}\")\n",
    "print(f\"Validation scores: {cv_results_best['test_score']}\")\n",
    "print(f\"\\nMean train accuracy: {np.mean(cv_results_best['train_score']):.4f}\")\n",
    "print(f\"Mean validation accuracy: {np.mean(cv_results_best['test_score']):.4f}\")\n",
    "print(f\"Std validation accuracy: {np.std(cv_results_best['test_score']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981f803",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Our model achieved a 54.10% up/down accuracy, which is sufficiently above randomness to suggest it is not merely producing noise and could form the basis of a profitable trading strategy with the right infrastructure. There is also roughly a 10% gap between training and validation accuracy, which is acceptable and significantly lower than our original figures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
